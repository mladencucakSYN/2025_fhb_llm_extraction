---
title: "Batch Processing with Rate Limiting"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(knitr)
```

## Overview

This notebook demonstrates batch processing for extracting data from large numbers of studies with rate limiting and progress tracking.

## Load Packages and Data

```{r load-all}
library(dplyr)

source("../R/gemini_extraction.R")
source("../R/retry_logic.R")
source("../R/batch_processor.R")
source("../R/utils.R")

# Model configuration - change here to switch models
# Options: "gemini-2.5-flash-lite" (fast), "gemini-2.5-pro" (better quality)
GEMINI_MODEL <- "gemini-2.5-flash-lite"

data_dir <- "../data/fusarium"
```

```{r load-sample}
sample_50 <- readRDS(file.path(data_dir, "sample_50_studies.rds"))

tibble(
  Dataset = "50-study sample",
  Studies = nrow(sample_50)
) %>% kable()
```

## Define Extraction Function

```{r define-extractor}
extract_one_study <- function(study_row) {
  result <- tryCatch({
    retry_with_backoff({
      extract_fusarium_gemini(
<<<<<<< HEAD
        abstract = safe_text(study_row$abstract_clean),
        title = safe_text(study_row$title),
        keywords = safe_text(study_row$keywords_clean)
=======
        abstract = study_row$abstract_clean,
        title = study_row$title,
        keywords = study_row$keywords_clean,
        model = GEMINI_MODEL
>>>>>>> origin/main
      )
    }, max_attempts = 3, base_delay = 2)
  }, error = function(e) {
    NULL
  })

  if (!is.null(result)) {
    result <- lapply(result, safe_text)
    result$id <- study_row$id
    as.data.frame(result, stringsAsFactors = FALSE)
  } else {
    NULL
  }
}
```

## Run Batch Processing

Process studies in batches with rate limiting.

```{r run-batch}
demo_sample <- sample_50[1:10, ]

results_df <- process_batch(
  docs = demo_sample,
  extract_fn = extract_one_study,
  id_col = "id",
  batch_size = 5,
  delay_seconds = 10,
  verbose = TRUE
)
```

## Processing Results

```{r examine-results}
tibble(
  Metric = c("Total processed", "Success rate"),
  Value = c(nrow(results_df), paste0(round(nrow(results_df) / nrow(demo_sample) * 100), "%"))
) %>% kable()
```

## Sample Extracted Data

```{r sample-data}
if (nrow(results_df) > 0) {
  results_df %>%
    head(3) %>%
    mutate(
      species = sapply(fusarium_species, function(x) paste(x, collapse = ", ")),
      crops = sapply(crop, function(x) paste(x, collapse = ", "))
    ) %>%
    select(id, species, crops, modeling) %>%
    kable()
}
```

## Save Results

```{r save-results}
results_file <- file.path(data_dir, "batch_extraction_results_demo.rds")
saveRDS(results_df, results_file)

tibble(
  Output = "Batch results",
  Path = results_file,
  Records = nrow(results_df)
) %>% kable()
```

## Batch Processing Features

```{r features}
tibble(
  Feature = c(
    "Rate limiting",
    "Progress tracking",
    "Error handling per document",
    "Automatic delays between batches",
    "Results aggregation"
  ),
  Description = c(
    "Respects API rate limits",
    "Shows progress during processing",
    "Continues on individual failures",
    "Configurable delay between batches",
    "Combines all results into data frame"
  )
) %>% kable()
```

## Next Steps

Proceed to `0410_caching_strategy.Rmd` for caching implementation.
