---
title: "Preprocess Abstracts for Extraction"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(knitr)
```

## Overview

This notebook preprocesses Fusarium study abstracts for LLM extraction by cleaning text, combining fields, and calculating quality metrics.

## Load Packages

```{r load-packages}
library(dplyr)
library(stringr)
library(readr)
library(ggplot2)
library(tidyr)

source("../R/utils.R")
```

## Load Data

```{r load-data}
data_dir <- "../data/fusarium"
data_file <- file.path(data_dir, "fusarium_studies_loaded.rds")

fusarium_data <- readRDS(data_file)

tibble(
  Metric = c("Records loaded", "Source file"),
  Value = c(nrow(fusarium_data), data_file)
) %>% kable()
```

## Text Quality Assessment

Identify text issues before cleaning.

```{r text-issues}
text_issues <- tibble(
  Issue = c("Contains newlines", "Contains tabs", "Multiple spaces", "Non-ASCII characters"),
  `Affected Abstracts` = c(
    sum(str_detect(fusarium_data$abstract, "\n"), na.rm = TRUE),
    sum(str_detect(fusarium_data$abstract, "\t"), na.rm = TRUE),
    sum(str_detect(fusarium_data$abstract, "  +"), na.rm = TRUE),
    sum(str_detect(fusarium_data$abstract, "[^ -~]"), na.rm = TRUE)
  ),
  Percentage = round(`Affected Abstracts` / nrow(fusarium_data) * 100, 1)
)

text_issues %>% kable()
```

## Define Cleaning Function

```{r clean-function}
clean_abstract_text <- function(text) {
  result <- text
  result <- ifelse(is.na(result) | nchar(result) == 0, "", result)

  result <- result %>%
    str_replace_all("[\r\n\t]+", " ") %>%
    str_replace_all("\\s+", " ") %>%
    str_trim() %>%
    str_replace_all("[\u2018\u2019]", "'") %>%
    str_replace_all("[\u201c\u201d]", '"') %>%
    str_replace_all("\u2013|\u2014", "-") %>%
    str_remove_all("[[:cntrl:]]")

  return(result)
}
```

## Apply Text Cleaning

```{r apply-cleaning}
fusarium_data <- fusarium_data %>%
  mutate(
    title_clean = clean_abstract_text(title),
    abstract_clean = clean_abstract_text(abstract),
    keywords_clean = clean_abstract_text(keywords)
  )
```

## Combine Text Fields

Create combined text field with title, abstract, and keywords for extraction.

```{r combine-text}
fusarium_data <- fusarium_data %>%
  mutate(
    combined_text = paste(
      ifelse(nchar(title_clean) > 0, paste("Title:", title_clean), ""),
      ifelse(nchar(abstract_clean) > 0, paste("Abstract:", abstract_clean), ""),
      ifelse(nchar(keywords_clean) > 0, paste("Keywords:", keywords_clean), ""),
      sep = "\n\n"
    ),
    combined_text = str_trim(combined_text),
    combined_length = nchar(combined_text)
  )
```

## Handle Missing Text

```{r handle-missing}
fusarium_data <- fusarium_data %>%
  mutate(
    has_title = nchar(title_clean) > 0,
    has_abstract = nchar(abstract_clean) > 0,
    has_keywords = nchar(keywords_clean) > 0,
    has_sufficient_text = combined_length >= 50,
    exclude_from_extraction = !has_abstract | !has_sufficient_text
  )

tibble(
  Field = c("Missing title", "Missing abstract", "Missing keywords", "Insufficient text (<50 chars)"),
  Count = c(
    sum(!fusarium_data$has_title),
    sum(!fusarium_data$has_abstract),
    sum(!fusarium_data$has_keywords),
    sum(!fusarium_data$has_sufficient_text)
  ),
  Percentage = round(Count / nrow(fusarium_data) * 100, 1)
) %>% kable()
```

## Extraction Eligibility

```{r eligibility}
tibble(
  Status = c("Valid for extraction", "Excluded"),
  Count = c(
    sum(!fusarium_data$exclude_from_extraction),
    sum(fusarium_data$exclude_from_extraction)
  ),
  Percentage = round(Count / nrow(fusarium_data) * 100, 1)
) %>% kable()
```

## Text Length Distribution

```{r length-analysis, fig.width=10, fig.height=5}
valid_data <- fusarium_data %>% filter(!exclude_from_extraction)

tibble(
  Statistic = c("Min", "Q25", "Median", "Mean", "Q75", "Max"),
  `Combined Length` = c(
    min(valid_data$combined_length),
    quantile(valid_data$combined_length, 0.25),
    median(valid_data$combined_length),
    round(mean(valid_data$combined_length)),
    quantile(valid_data$combined_length, 0.75),
    max(valid_data$combined_length)
  )
) %>% kable()

ggplot(valid_data, aes(x = combined_length)) +
  geom_histogram(bins = 40, fill = "skyblue", alpha = 0.7) +
  geom_vline(aes(xintercept = median(combined_length)),
             color = "red", linetype = "dashed", linewidth = 1) +
  labs(
    title = "Distribution of Combined Text Lengths",
    subtitle = paste("Median:", median(valid_data$combined_length), "characters"),
    x = "Text Length (characters)",
    y = "Number of Studies"
  ) +
  theme_minimal()
```

## Token Estimation

Estimate token counts for API cost planning (approx. 1 token per 4 characters).

```{r token-estimation}
fusarium_data <- fusarium_data %>%
  mutate(estimated_tokens = ceiling(combined_length / 4))

valid_data <- fusarium_data %>% filter(!exclude_from_extraction)

tibble(
  Metric = c("Mean tokens per study", "Median tokens per study", "Total tokens", "Valid studies"),
  Value = c(
    round(mean(valid_data$estimated_tokens)),
    median(valid_data$estimated_tokens),
    format(sum(valid_data$estimated_tokens), big.mark = ","),
    nrow(valid_data)
  )
) %>% kable()
```

## Quality Metrics

Calculate relevance indicators based on key terms.

```{r quality-metrics}
fusarium_data <- fusarium_data %>%
  mutate(
    word_count = str_count(combined_text, "\\S+"),
    sentence_count = str_count(combined_text, "[.!?]+"),
    mentions_fusarium = str_detect(combined_text, regex("fusarium", ignore_case = TRUE)),
    mentions_wheat = str_detect(combined_text, regex("wheat|barley|oat|cereal", ignore_case = TRUE)),
    mentions_climate = str_detect(combined_text, regex("climate|temperature|moisture|humidity", ignore_case = TRUE)),
    quality_score = as.integer(mentions_fusarium) +
                    as.integer(mentions_wheat) +
                    as.integer(mentions_climate)
  )

valid_data <- fusarium_data %>% filter(!exclude_from_extraction)

tibble(
  `Quality Indicator` = c("Mentions Fusarium", "Mentions cereals", "Mentions climate factors"),
  Count = c(
    sum(valid_data$mentions_fusarium),
    sum(valid_data$mentions_wheat),
    sum(valid_data$mentions_climate)
  ),
  Percentage = round(c(
    mean(valid_data$mentions_fusarium),
    mean(valid_data$mentions_wheat),
    mean(valid_data$mentions_climate)
  ) * 100, 1)
) %>% kable()
```

## Quality Score Distribution

```{r quality-distribution, fig.width=8, fig.height=4}
quality_dist <- valid_data %>%
  count(quality_score, name = "n_studies") %>%
  mutate(percentage = round(n_studies / sum(n_studies) * 100, 1))

ggplot(quality_dist, aes(x = factor(quality_score), y = n_studies)) +
  geom_col(fill = "steelblue") +
  geom_text(aes(label = paste0(n_studies, "\n(", percentage, "%)")),
            vjust = -0.2, size = 3) +
  labs(
    title = "Quality Score Distribution",
    subtitle = "Score based on mentions of Fusarium, cereals, and climate factors",
    x = "Quality Score (0-3)",
    y = "Number of Studies"
  ) +
  theme_minimal()

quality_dist %>% kable()
```

## Save Preprocessed Data

```{r save-preprocessed}
processed_file <- file.path(data_dir, "fusarium_studies_preprocessed.rds")
saveRDS(fusarium_data, processed_file)

valid_data <- fusarium_data %>% filter(!exclude_from_extraction)
valid_file <- file.path(data_dir, "fusarium_studies_extraction_ready.rds")
saveRDS(valid_data, valid_file)

summary_data <- fusarium_data %>%
  select(id, title, combined_length, word_count, quality_score,
         exclude_from_extraction, mentions_fusarium, mentions_wheat, mentions_climate)
summary_file <- file.path(data_dir, "preprocessing_summary.csv")
write_csv(summary_data, summary_file)

tibble(
  Output = c("Full preprocessed data", "Extraction-ready subset", "Summary CSV"),
  Path = c(processed_file, valid_file, summary_file),
  Records = c(nrow(fusarium_data), nrow(valid_data), nrow(summary_data))
) %>% kable()
```

## Summary

```{r final-summary}
tibble(
  Metric = c(
    "Total records",
    "Valid for extraction",
    "Excluded",
    "Mean text length (valid)",
    "Total estimated tokens"
  ),
  Value = c(
    nrow(fusarium_data),
    sum(!fusarium_data$exclude_from_extraction),
    sum(fusarium_data$exclude_from_extraction),
    paste(round(mean(valid_data$combined_length)), "chars"),
    format(sum(valid_data$estimated_tokens), big.mark = ",")
  )
) %>% kable()
```

## Next Steps

Proceed to `0120_exploratory_analysis.Rmd` for detailed corpus analysis.
