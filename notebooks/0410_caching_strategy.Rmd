---
title: "Caching Strategy for Large-Scale Extraction"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(knitr)
```

## Overview

This notebook implements caching to save extraction results and enable resuming from failures.

## Load Packages

```{r load-all}
library(dplyr)

source("../R/gemini_extraction.R")
source("../R/retry_logic.R")
source("../R/cache_manager.R")

data_dir <- "../data/fusarium"
cache_dir <- "../data/cache"
```

## Test Cache Functions

```{r test-cache}
<<<<<<< HEAD
cat("=== Testing Cache Functions ===\n\n")
unlink(cache_dir, recursive = TRUE)

# Create a test extraction result
=======
>>>>>>> origin/main
test_result <- list(
  id = "TEST_001",
  fusarium_species = safe_text(c("F. graminearum")),
  crop = safe_text(c("wheat")),
  summary = "Test extraction result"
)

cache_extraction("TEST_001", test_result, cache_dir)

loaded_results <- load_cached_extractions(cache_dir)
stats <- cache_stats(cache_dir)

tibble(
  Metric = c("Cached files", "Cache size"),
  Value = c(stats$n_files, paste(stats$size_mb, "MB"))
) %>% kable()
```

## Process with Caching

```{r process-with-cache}
sample_10 <- readRDS(file.path(data_dir, "sample_10_studies.rds"))
demo_size <- min(5, nrow(sample_10))

results <- list()
process_log <- tibble(
  Study = integer(),
  ID = character(),
  Status = character()
)

for (i in seq_len(demo_size)) {
  study <- sample_10[i, ]
  doc_id <- study$id

  result <- tryCatch({
    retry_with_backoff({
      extract_fusarium_gemini(
        abstract = study$abstract_clean,
        title = study$title,
        keywords = safe_text(study$keywords_clean)
      )
    }, max_attempts = 3, base_delay = 2)
  }, error = function(e) {
    NULL
  })

  if (!is.null(result)) {
    result$id <- doc_id
    cache_extraction(doc_id, result, cache_dir)
    results[[i]] <- result
    process_log <- bind_rows(process_log, tibble(
      Study = i, ID = doc_id, Status = "Cached"
    ))
  } else {
    process_log <- bind_rows(process_log, tibble(
      Study = i, ID = doc_id, Status = "Failed"
    ))
  }

  Sys.sleep(2)
}

process_log %>% kable()
```

## Cache Status

```{r cache-status}
all_docs <- sample_10
uncached <- get_uncached_docs(all_docs, id_col = "id", cache_dir = cache_dir)

tibble(
  Metric = c("Total documents", "Cached", "Remaining"),
  Count = c(
    nrow(all_docs),
    nrow(all_docs) - nrow(uncached),
    nrow(uncached)
  )
) %>% kable()
```

## Resume Capability

```{r demonstrate-resume}
cached_results <- load_cached_extractions(cache_dir)
remaining <- get_uncached_docs(sample_10[1:10, ], id_col = "id", cache_dir = cache_dir)

tibble(
  Metric = c("Already processed", "Remaining to process"),
  Count = c(nrow(cached_results), nrow(remaining))
) %>% kable()
```

## Caching Benefits

```{r benefits}
tibble(
  Benefit = c(
    "Resume from failures",
    "Avoid re-processing",
    "Save API costs",
    "Track progress",
    "Incremental extraction"
  ),
  Description = c(
    "Continue where interrupted",
    "Skip already extracted documents",
    "No duplicate API calls",
    "Know what's done and what's left",
    "Process in multiple sessions"
  )
) %>% kable()
```

## Next Steps

Proceed to `0420_run_full_extraction.Rmd` for full-scale extraction.
